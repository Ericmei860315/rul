{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RUL_Net.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWFpNXKVbudv",
        "outputId": "76bfd2a8-bbd9-4e38-9212-3bf8c4f659b1"
      },
      "source": [
        "!pip install tensorflow==1.14.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/28/96efba1a516cdacc2e2d6d081f699c001d414cc8ca3250e6d59ae657eb2b/tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3MB)\n",
            "\u001b[K     |████████████████████████████████| 109.3MB 100kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.19.5)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.36.2)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 47.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (3.12.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 46.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.32.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Collecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (53.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.0)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCOe-OWCeSbj",
        "outputId": "219e73b0-805f-48df-a345-077a8f9d8431"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "print(tf.__version__)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7YPrwx2Nzfu"
      },
      "source": [
        "# data_processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fnipemsJ7DP"
      },
      "source": [
        "import pandas as pd\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "import random\r\n",
        "\r\n",
        "MAXLIFE = 120\r\n",
        "SCALE = 1\r\n",
        "RESCALE = 1\r\n",
        "true_rul = []\r\n",
        "test_engine_id = 0\r\n",
        "training_engine_id = 0"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1tlFofeJ9XA"
      },
      "source": [
        "def kink_RUL(cycle_list, max_cycle):\r\n",
        "    '''\r\n",
        "    Piecewise linear function with zero gradient and unit gradient\r\n",
        "\r\n",
        "            ^\r\n",
        "            |\r\n",
        "    MAXLIFE |-----------\r\n",
        "            |            \\\r\n",
        "            |             \\\r\n",
        "            |              \\\r\n",
        "            |               \\\r\n",
        "            |                \\\r\n",
        "            |----------------------->\r\n",
        "    '''\r\n",
        "    knee_point = max_cycle - MAXLIFE\r\n",
        "    kink_RUL = []\r\n",
        "    stable_life = MAXLIFE\r\n",
        "    for i in range(0, len(cycle_list)):\r\n",
        "        if i < knee_point:\r\n",
        "            kink_RUL.append(MAXLIFE)\r\n",
        "        else:\r\n",
        "            tmp = kink_RUL[i - 1] - (stable_life / (max_cycle - knee_point))\r\n",
        "            kink_RUL.append(tmp)\r\n",
        "\r\n",
        "    return kink_RUL\r\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGfrKNuzKA8u"
      },
      "source": [
        "def compute_rul_of_one_id(FD00X_of_one_id, max_cycle_rul=None):\r\n",
        "    '''\r\n",
        "    Enter the data of an engine_id of train_FD001 and output the corresponding RUL (remaining life) of these data.\r\n",
        "    type is list\r\n",
        "    '''\r\n",
        "\r\n",
        "    cycle_list = FD00X_of_one_id['cycle'].tolist()\r\n",
        "    if max_cycle_rul is None:\r\n",
        "        max_cycle = max(cycle_list)  # Failure cycle\r\n",
        "    else:\r\n",
        "        max_cycle = max(cycle_list) + max_cycle_rul\r\n",
        "        # print(max(cycle_list), max_cycle_rul)\r\n",
        "\r\n",
        "    # return kink_RUL(cycle_list,max_cycle)\r\n",
        "    return kink_RUL(cycle_list, max_cycle)\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIEAlC0UKP-F"
      },
      "source": [
        "def compute_rul_of_one_file(FD00X, id='engine_id', RUL_FD00X=None):\r\n",
        "    '''\r\n",
        "    Input train_FD001, output a list\r\n",
        "    '''\r\n",
        "    rul = []\r\n",
        "    # In the loop train, each id value of the 'engine_id' column\r\n",
        "    if RUL_FD00X is None:\r\n",
        "        for _id in set(FD00X[id]):\r\n",
        "            rul.extend(compute_rul_of_one_id(FD00X[FD00X[id] == _id]))\r\n",
        "        return rul\r\n",
        "    else:\r\n",
        "        rul = []\r\n",
        "        for _id in set(FD00X[id]):\r\n",
        "            # print(\"#### id ####\", int(RUL_FD00X.iloc[_id - 1]))\r\n",
        "            true_rul.append(int(RUL_FD00X.iloc[_id - 1]))\r\n",
        "            rul.extend(compute_rul_of_one_id(FD00X[FD00X[id] == _id], int(RUL_FD00X.iloc[_id - 1])))\r\n",
        "        return rul\r\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVV3PZcsKTu8"
      },
      "source": [
        "def get_CMAPSSData(save=False, save_training_data=True, save_testing_data=True, files=[1, 2, 3, 4, 5],\r\n",
        "                   min_max_norm=False):\r\n",
        "    '''\r\n",
        "    :param save: switch to load the already preprocessed data or begin preprocessing of raw data\r\n",
        "    :param save_training_data: same functionality as 'save' but for training data only\r\n",
        "    :param save_testing_data: same functionality as 'save' but for testing data only\r\n",
        "    :param files: to indicate which sub dataset needed to be loaded for operations\r\n",
        "    :param min_max_norm: switch to enable min-max normalization\r\n",
        "    :return: function will save the preprocessed training and testing data as numpy objects\r\n",
        "    '''\r\n",
        "\r\n",
        "    if save == False:\r\n",
        "        return np.load(\"normalized_train_data.npy\"), np.load(\"normalized_test_data.npy\"), pd.read_csv(\r\n",
        "            'normalized_train_data.csv', index_col=[0]), pd.read_csv('normalized_test_data.csv', index_col=[0])\r\n",
        "\r\n",
        "    column_name = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\r\n",
        "                   's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\r\n",
        "                   's15', 's16', 's17', 's18', 's19', 's20', 's21']\r\n",
        "\r\n",
        "    if save_training_data:  ### Training ###\r\n",
        "\r\n",
        "        train_FD001 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/train_FD001.txt\", header=None, delim_whitespace=True)\r\n",
        "        train_FD002 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/train_FD002.txt\", header=None, delim_whitespace=True)\r\n",
        "        train_FD003 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/train_FD003.txt\", header=None, delim_whitespace=True)\r\n",
        "        train_FD004 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/train_FD004.txt\", header=None, delim_whitespace=True)\r\n",
        "        train_FD001.columns = column_name\r\n",
        "        train_FD002.columns = column_name\r\n",
        "        train_FD003.columns = column_name\r\n",
        "        train_FD004.columns = column_name\r\n",
        "\r\n",
        "        previous_len = 0\r\n",
        "        frames = []\r\n",
        "        for data_file in ['train_FD00' + str(i) for i in files]:  # load subdataset by subdataset\r\n",
        "\r\n",
        "            #### standard normalization ####\r\n",
        "            mean = eval(data_file).iloc[:, 2:len(list(eval(data_file)))].mean()\r\n",
        "            std = eval(data_file).iloc[:, 2:len(list(eval(data_file)))].std()\r\n",
        "            std.replace(0, 1, inplace=True)\r\n",
        "            # print(\"std\", std)\r\n",
        "            ################################\r\n",
        "\r\n",
        "            if min_max_norm:\r\n",
        "                scaler = MinMaxScaler()\r\n",
        "                eval(data_file).iloc[:, 2:len(list(eval(data_file)))] = scaler.fit_transform(\r\n",
        "                    eval(data_file).iloc[:, 2:len(list(eval(data_file)))])\r\n",
        "            else:\r\n",
        "                eval(data_file).iloc[:, 2:len(list(eval(data_file)))] = (eval(data_file).iloc[:, 2:len(\r\n",
        "                    list(eval(data_file)))] - mean) / std\r\n",
        "\r\n",
        "            eval(data_file)['RUL'] = compute_rul_of_one_file(eval(data_file))\r\n",
        "            current_len = len(eval(data_file))\r\n",
        "            # print(eval(data_file).index)\r\n",
        "            eval(data_file).index = range(previous_len, previous_len + current_len)\r\n",
        "            previous_len = previous_len + current_len\r\n",
        "            # print(eval(data_file).index)\r\n",
        "            frames.append(eval(data_file))\r\n",
        "            print(data_file)\r\n",
        "\r\n",
        "        train = pd.concat(frames)\r\n",
        "        global training_engine_id\r\n",
        "        training_engine_id = train['engine_id']\r\n",
        "        train = train.drop('engine_id', 1)\r\n",
        "        train = train.drop('cycle', 1)\r\n",
        "        # if files[0] == 1 or files[0] == 3:\r\n",
        "        #     train = train.drop('setting3', 1)\r\n",
        "        #     train = train.drop('s18', 1)\r\n",
        "        #     train = train.drop('s19', 1)\r\n",
        "\r\n",
        "        train_values = train.values * SCALE\r\n",
        "        np.save('normalized_train_data.npy', train_values)\r\n",
        "        train.to_csv('normalized_train_data.csv')\r\n",
        "        ###########\r\n",
        "    else:\r\n",
        "        train = pd.read_csv('normalized_train_data.csv', index_col=[0])\r\n",
        "        train_values = train.values\r\n",
        "\r\n",
        "    if save_testing_data:  ### testing ###\r\n",
        "\r\n",
        "        test_FD001 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/test_FD001.txt\", header=None, delim_whitespace=True)\r\n",
        "        test_FD002 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/test_FD001.txt\", header=None, delim_whitespace=True)\r\n",
        "        test_FD003 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/test_FD001.txt\", header=None, delim_whitespace=True)\r\n",
        "        test_FD004 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/test_FD001.txt\", header=None, delim_whitespace=True)\r\n",
        "        test_FD001.columns = column_name\r\n",
        "        test_FD002.columns = column_name\r\n",
        "        test_FD003.columns = column_name\r\n",
        "        test_FD004.columns = column_name\r\n",
        "\r\n",
        "        # load RUL data\r\n",
        "        RUL_FD001 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/RUL_FD001.txt\", header=None, delim_whitespace=True)\r\n",
        "        RUL_FD002 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/RUL_FD002.txt\", header=None, delim_whitespace=True)\r\n",
        "        RUL_FD003 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/RUL_FD003.txt\", header=None, delim_whitespace=True)\r\n",
        "        RUL_FD004 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/RUL_FD004.txt\", header=None, delim_whitespace=True)\r\n",
        "        RUL_FD001.columns = ['RUL']\r\n",
        "        RUL_FD002.columns = ['RUL']\r\n",
        "        RUL_FD003.columns = ['RUL']\r\n",
        "        RUL_FD004.columns = ['RUL']\r\n",
        "\r\n",
        "        previous_len = 0\r\n",
        "        frames = []\r\n",
        "        for (data_file, rul_file) in [('test_FD00' + str(i), 'RUL_FD00' + str(i)) for i in files]:\r\n",
        "            mean = eval(data_file).iloc[:, 2:len(list(eval(data_file)))].mean()\r\n",
        "            std = eval(data_file).iloc[:, 2:len(list(eval(data_file)))].std()\r\n",
        "            std.replace(0, 1, inplace=True)\r\n",
        "\r\n",
        "            if min_max_norm:\r\n",
        "                scaler = MinMaxScaler()\r\n",
        "                eval(data_file).iloc[:, 2:len(list(eval(data_file)))] = scaler.fit_transform(\r\n",
        "                    eval(data_file).iloc[:, 2:len(list(eval(data_file)))])\r\n",
        "            else:\r\n",
        "                eval(data_file).iloc[:, 2:len(list(eval(data_file)))] = (eval(data_file).iloc[:, 2:len(\r\n",
        "                    list(eval(data_file)))] - mean) / std\r\n",
        "\r\n",
        "            eval(data_file)['RUL'] = compute_rul_of_one_file(eval(data_file), RUL_FD00X=eval(rul_file))\r\n",
        "            current_len = len(eval(data_file))\r\n",
        "            eval(data_file).index = range(previous_len, previous_len + current_len)\r\n",
        "            previous_len = previous_len + current_len\r\n",
        "            frames.append(eval(data_file))\r\n",
        "            print(data_file)\r\n",
        "            if len(files) == 1:\r\n",
        "                global test_engine_id\r\n",
        "                test_engine_id = eval(data_file)['engine_id']\r\n",
        "\r\n",
        "        test = pd.concat(frames)\r\n",
        "        test = test.drop('engine_id', 1)\r\n",
        "        test = test.drop('cycle', 1)\r\n",
        "        # if files[0] == 1 or files[0] == 3:\r\n",
        "        #     test = test.drop('setting3', 1)\r\n",
        "        #     test = test.drop('s18', 1)\r\n",
        "        #     test = test.drop('s19', 1)\r\n",
        "\r\n",
        "        test_values = test.values * SCALE\r\n",
        "        np.save('normalized_test_data.npy', test_values)\r\n",
        "        test.to_csv('normalized_test_data.csv')\r\n",
        "        ###########\r\n",
        "    else:\r\n",
        "        test = pd.read_csv('normalized_test_data.csv', index_col=[0])\r\n",
        "        test_values = test.values\r\n",
        "\r\n",
        "    return train_values, test_values, train, test"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90GPYE1rLHVj"
      },
      "source": [
        "def get_PHM08Data(save=False):\r\n",
        "    \"\"\"\r\n",
        "    Function is to load PHM 2008 challenge dataset\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    if save == False:\r\n",
        "        return np.load(\"./PHM08/processed_data/phm_training_data.npy\"), np.load(\"./PHM08/processed_data/phm_testing_data.npy\"), np.load(\r\n",
        "            \"./PHM08/processed_data/phm_original_testing_data.npy\")\r\n",
        "\r\n",
        "    column_name = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\r\n",
        "                   's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\r\n",
        "                   's15', 's16', 's17', 's18', 's19', 's20', 's21']\r\n",
        "    phm_training_data = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/Challenge_Data/train.txt\", header=None, delim_whitespace=True)\r\n",
        "    phm_training_data.columns = column_name\r\n",
        "    phm_testing_data = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/Challenge_Data/final_test.txt\", header=None, delim_whitespace=True)\r\n",
        "    phm_testing_data.columns = column_name\r\n",
        "\r\n",
        "    print(\"phm training\")\r\n",
        "    mean = phm_training_data.iloc[:, 2:len(list(phm_training_data))].mean()\r\n",
        "    std = phm_training_data.iloc[:, 2:len(list(phm_training_data))].std()\r\n",
        "    phm_training_data.iloc[:, 2:len(list(phm_training_data))] = (phm_training_data.iloc[:, 2:len(\r\n",
        "        list(phm_training_data))] - mean) / std\r\n",
        "    phm_training_data['RUL'] = compute_rul_of_one_file(phm_training_data)\r\n",
        "\r\n",
        "    print(\"phm testing\")\r\n",
        "    mean = phm_testing_data.iloc[:, 2:len(list(phm_testing_data))].mean()\r\n",
        "    std = phm_testing_data.iloc[:, 2:len(list(phm_testing_data))].std()\r\n",
        "    phm_testing_data.iloc[:, 2:len(list(phm_testing_data))] = (phm_testing_data.iloc[:, 2:len(\r\n",
        "        list(phm_testing_data))] - mean) / std\r\n",
        "    phm_testing_data['RUL'] = 0\r\n",
        "    #phm_testing_data['RUL'] = compute_rul_of_one_file(phm_testing_data)\r\n",
        "\r\n",
        "    train_engine_id = phm_training_data['engine_id']\r\n",
        "    # print(phm_training_engine_id[phm_training_engine_id==1].index)\r\n",
        "    phm_training_data = phm_training_data.drop('engine_id', 1)\r\n",
        "    phm_training_data = phm_training_data.drop('cycle', 1)\r\n",
        "\r\n",
        "    global test_engine_id\r\n",
        "    test_engine_id = phm_testing_data['engine_id']\r\n",
        "    phm_testing_data = phm_testing_data.drop('engine_id', 1)\r\n",
        "    phm_testing_data = phm_testing_data.drop('cycle', 1)\r\n",
        "\r\n",
        "    phm_training_data = phm_training_data.values\r\n",
        "    phm_testing_data = phm_testing_data.values\r\n",
        "\r\n",
        "    engine_ids = train_engine_id.unique()\r\n",
        "    train_test_split = np.random.rand(len(engine_ids)) < 0.80\r\n",
        "    train_engine_ids = engine_ids[train_test_split]\r\n",
        "    test_engine_ids = engine_ids[~train_test_split]\r\n",
        "\r\n",
        "    # test_engine_id = pd.Series(test_engine_ids)\r\n",
        "\r\n",
        "\r\n",
        "    training_data = phm_training_data[train_engine_id[train_engine_id == train_engine_ids[0]].index]\r\n",
        "    for id in train_engine_ids[1:]:\r\n",
        "        tmp = phm_training_data[train_engine_id[train_engine_id == id].index]\r\n",
        "        training_data = np.concatenate((training_data, tmp))\r\n",
        "    # print(training_data.shape)\r\n",
        "\r\n",
        "    testing_data = phm_training_data[train_engine_id[train_engine_id == test_engine_ids[0]].index]\r\n",
        "    for id in test_engine_ids[1:]:\r\n",
        "        tmp = phm_training_data[train_engine_id[train_engine_id == id].index]\r\n",
        "        testing_data = np.concatenate((testing_data, tmp))\r\n",
        "    # print(testing_data.shape)\r\n",
        "\r\n",
        "    print(phm_training_data.shape, phm_testing_data.shape, training_data.shape, testing_data.shape)\r\n",
        "\r\n",
        "    np.save(\"./PHM08/processed_data/phm_training_data.npy\", training_data)\r\n",
        "    np.savetxt(\"./PHM08/processed_data/phm_training_data.txt\", training_data, delimiter=\" \")\r\n",
        "    np.save(\"./PHM08/processed_data/phm_testing_data.npy\", testing_data)\r\n",
        "    np.savetxt(\"./PHM08/processed_data/phm_testing_data.txt\", testing_data, delimiter=\" \")\r\n",
        "    np.save(\"./PHM08/processed_data/phm_original_testing_data.npy\", phm_testing_data)\r\n",
        "    np.savetxt(\"./PHM08/processed_data/phm_original_testing_data.csv\", phm_testing_data, delimiter=\",\")\r\n",
        "\r\n",
        "    return training_data, testing_data, phm_testing_data"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s41xJ0sXLtX3"
      },
      "source": [
        "def data_augmentation(files=1, low=[10, 40, 90, 170], high=[35, 85, 160, 250], plot=False, combine=False):\r\n",
        "    '''\r\n",
        "    This helper function only augments the training data to look like testing data.\r\n",
        "    Training data always run to a failure. But testing data is mostly stop before a failure.\r\n",
        "    Therefore, training data augmented to have scenarios without failure\r\n",
        "\r\n",
        "    :param files: select wich sub CMPASS dataset\r\n",
        "    :param low: lower bound for the random selection of the engine cycle\r\n",
        "    :param high: upper bound for the random selection of the engine cycle\r\n",
        "    :param plot: switch to plot the augmented data\r\n",
        "    :return:\r\n",
        "    '''\r\n",
        "\r\n",
        "    DEBUG = False\r\n",
        "\r\n",
        "    column_name = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\r\n",
        "                   's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\r\n",
        "                   's15', 's16', 's17', 's18', 's19', 's20', 's21']\r\n",
        "\r\n",
        "    ### Loading original data ###\r\n",
        "    if files == \"phm\":\r\n",
        "        train_FD00x = pd.read_table(\"./PHM08/processed_data/phm_training_data.txt\", header=None, delim_whitespace=True)\r\n",
        "        train_FD00x.drop(train_FD00x.columns[len(train_FD00x.columns) - 1], axis=1, inplace=True)\r\n",
        "        train_FD00x.columns = column_name\r\n",
        "    else:\r\n",
        "        if combine:\r\n",
        "            train_FD00x,_,_ = combine_FD001_and_FD003()\r\n",
        "        else:\r\n",
        "            file_path = \"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/train_FD00\" + str(files) + \".txt\"\r\n",
        "            train_FD00x = pd.read_table(file_path, header=None, delim_whitespace=True)\r\n",
        "            train_FD00x.columns = column_name\r\n",
        "            print(file_path.split(\"/\")[-1])\r\n",
        "\r\n",
        "        ### Standered Normal ###\r\n",
        "        mean = train_FD00x.iloc[:, 2:len(list(train_FD00x))].mean()\r\n",
        "        std = train_FD00x.iloc[:, 2:len(list(train_FD00x))].std()\r\n",
        "        std.replace(0, 1, inplace=True)\r\n",
        "        train_FD00x.iloc[:, 2:len(list(train_FD00x))] = (train_FD00x.iloc[:, 2:len(list(train_FD00x))] - mean) / std\r\n",
        "\r\n",
        "    final_train_FD = train_FD00x.copy()\r\n",
        "    previous_len = 0\r\n",
        "    frames = []\r\n",
        "    for i in range(len(high)):\r\n",
        "        train_FD = train_FD00x.copy()\r\n",
        "        train_engine_id = train_FD['engine_id']\r\n",
        "        engine_ids = train_engine_id.unique()\r\n",
        "        total_ids = len(engine_ids)\r\n",
        "        train_rul = []\r\n",
        "        print(\"*************\", final_train_FD.shape, total_ids, low[i], high[i], \"*****************\")\r\n",
        "\r\n",
        "        for id in range(1, total_ids + 1):\r\n",
        "\r\n",
        "            train_engine_id = train_FD['engine_id']\r\n",
        "            indexes = train_engine_id[train_engine_id == id].index  ### filter indexes related to id\r\n",
        "            traj_data = train_FD.loc[indexes]  ### filter trajectory data\r\n",
        "\r\n",
        "            cutoff_cycle = random.randint(low[i], high[i])  ### randomly selecting the cutoff point of the engine cycle\r\n",
        "\r\n",
        "            if cutoff_cycle > max(traj_data['cycle']):\r\n",
        "                cutoff_cycle = max(traj_data['cycle'])\r\n",
        "\r\n",
        "            train_rul.append(max(traj_data['cycle']) - cutoff_cycle)  ### collecting remaining cycles\r\n",
        "\r\n",
        "            cutoff_cycle_index = traj_data['cycle'][traj_data['cycle'] == cutoff_cycle].index  ### cutoff cycle index\r\n",
        "\r\n",
        "            if DEBUG:\r\n",
        "                print(\"traj_shape: \", traj_data.shape, \"current_engine_id:\", id, \"cutoff_cycle:\", cutoff_cycle,\r\n",
        "                      \"cutoff_index\", cutoff_cycle_index, \"engine_fist_index\", indexes[0], \"engine_last_index\",\r\n",
        "                      indexes[-1])\r\n",
        "\r\n",
        "            ### removing rows after cutoff cycle index ###\r\n",
        "            if cutoff_cycle_index[0] != indexes[-1]:\r\n",
        "                drop_range = list(range(cutoff_cycle_index[0] + 1, indexes[-1] + 1))\r\n",
        "                train_FD.drop(train_FD.index[drop_range], inplace=True)\r\n",
        "                train_FD.reset_index(drop=True, inplace=True)\r\n",
        "\r\n",
        "        ### calculating the RUL for augmented data\r\n",
        "        train_rul = pd.DataFrame.from_dict({'RUL': train_rul})\r\n",
        "        train_FD['RUL'] = compute_rul_of_one_file(train_FD, RUL_FD00X=train_rul)\r\n",
        "\r\n",
        "        ### changing the engine_id for augmented data\r\n",
        "        train_engine_id = train_FD['engine_id']\r\n",
        "        for id in range(1, total_ids + 1):\r\n",
        "            indexes = train_engine_id[train_engine_id == id].index\r\n",
        "            train_FD.loc[indexes, 'engine_id'] = id + total_ids * (i + 1)\r\n",
        "\r\n",
        "        if i == 0:  # should only execute at the first iteration\r\n",
        "            final_train_FD['RUL'] = compute_rul_of_one_file(final_train_FD)\r\n",
        "            current_len = len(final_train_FD)\r\n",
        "            final_train_FD.index = range(previous_len, previous_len + current_len)\r\n",
        "            previous_len = previous_len + current_len\r\n",
        "\r\n",
        "        ### Re-indexing the augmented data\r\n",
        "        train_FD['RUL'].index = range(previous_len, previous_len + len(train_FD))\r\n",
        "        previous_len = previous_len + len(train_FD)\r\n",
        "\r\n",
        "        final_train_FD = pd.concat(\r\n",
        "            [final_train_FD, train_FD])  # concatanete the newly augmented data with previous data\r\n",
        "\r\n",
        "    frames.append(final_train_FD)\r\n",
        "    train = pd.concat(frames)\r\n",
        "    train.reset_index(drop=True, inplace=True)\r\n",
        "\r\n",
        "    train_engine_id = train['engine_id']\r\n",
        "    # print(train_engine_id)\r\n",
        "    engine_ids = train_engine_id.unique()\r\n",
        "    # print(engine_ids[1:])\r\n",
        "    np.random.shuffle(engine_ids)\r\n",
        "    # print(engine_ids)\r\n",
        "\r\n",
        "    training_data = train.loc[train_engine_id[train_engine_id == engine_ids[0]].index]\r\n",
        "    training_data.reset_index(drop=True, inplace=True)\r\n",
        "    previous_len = len(training_data)\r\n",
        "    for id in engine_ids[1:]:\r\n",
        "        traj_data = train.loc[train_engine_id[train_engine_id == id].index]\r\n",
        "        current_len = len(traj_data)\r\n",
        "        traj_data.index = range(previous_len, previous_len + current_len)\r\n",
        "        previous_len = previous_len + current_len\r\n",
        "        training_data = pd.concat([training_data, traj_data])\r\n",
        "\r\n",
        "\r\n",
        "    global training_engine_id\r\n",
        "    training_engine_id = training_data['engine_id']\r\n",
        "\r\n",
        "    training_data = training_data.drop('engine_id', 1)\r\n",
        "    training_data = training_data.drop('cycle', 1)\r\n",
        "    # if files == 1 or files == 3:\r\n",
        "    #     training_data = training_data.drop('setting3', 1)\r\n",
        "    #     training_data = training_data.drop('s18', 1)\r\n",
        "    #     training_data = training_data.drop('s19', 1)\r\n",
        "\r\n",
        "    training_data_values = training_data.values * SCALE\r\n",
        "    np.save('normalized_train_data.npy', training_data_values)\r\n",
        "    training_data.to_csv('normalized_train_data.csv')\r\n",
        "\r\n",
        "\r\n",
        "    train = training_data_values\r\n",
        "    x_train = train[:, :train.shape[1] - 1]\r\n",
        "    y_train = train[:, train.shape[1] - 1] * RESCALE\r\n",
        "    print(\"training in augmentation\", x_train.shape, y_train.shape)\r\n",
        "\r\n",
        "    if plot:\r\n",
        "        plt.plot(y_train, label=\"train\")\r\n",
        "\r\n",
        "        plt.figure()\r\n",
        "        plt.plot(x_train)\r\n",
        "        plt.title(\"train\")\r\n",
        "        # plt.figure()\r\n",
        "        # plt.plot(y_train)\r\n",
        "        # plt.title(\"test\")\r\n",
        "\r\n",
        "        plt.show()\r\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrajig2TLx26"
      },
      "source": [
        "def analyse_Data(dataset, files=None, plot=True, min_max=False):\r\n",
        "    '''\r\n",
        "    Generate pre-processed data according to the given dataset\r\n",
        "    :param dataset: choose between \"phm\" for PHM 2008 dataset or \"cmapss\" for CMAPSS data set with file number\r\n",
        "    :param files: Only for CMAPSS dataset to select sub dataset\r\n",
        "    :param min_max: switch to allow min-max normalization\r\n",
        "    :return:\r\n",
        "    '''\r\n",
        "\r\n",
        "    if dataset == \"phm\":\r\n",
        "        training_data, testing_data, phm_testing_data = get_PHM08Data(save=True)\r\n",
        "\r\n",
        "        x_phmtrain = training_data[:, :training_data.shape[1] - 1]\r\n",
        "        y_phmtrain = training_data[:, training_data.shape[1] - 1]\r\n",
        "\r\n",
        "        x_phmtest = testing_data[:, :testing_data.shape[1] - 1]\r\n",
        "        y_phmtest = testing_data[:, testing_data.shape[1] - 1]\r\n",
        "\r\n",
        "        print(\"phmtrain\", x_phmtrain.shape, y_phmtrain.shape)\r\n",
        "\r\n",
        "        print(\"phmtest\", x_phmtrain.shape, y_phmtrain.shape)\r\n",
        "        print(\"phmtest\", phm_testing_data.shape)\r\n",
        "\r\n",
        "        if plot:\r\n",
        "            # plt.plot(x_phmtrain, label=\"phmtrain_x\")\r\n",
        "            plt.figure()\r\n",
        "            plt.plot(y_phmtrain, label=\"phmtrain_y\")\r\n",
        "\r\n",
        "            # plt.figure()\r\n",
        "            # plt.plot(x_phmtest, label=\"phmtest_x\")\r\n",
        "            plt.figure()\r\n",
        "            plt.plot(y_phmtest, label=\"phmtest_y\")\r\n",
        "\r\n",
        "            # plt.figure()\r\n",
        "            # plt.plot(phm_testing_data, label=\"test\")\r\n",
        "            plt.show()\r\n",
        "\r\n",
        "    elif dataset == \"cmapss\":\r\n",
        "        training_data, testing_data, training_pd, testing_pd = get_CMAPSSData(save=True, files=files,\r\n",
        "                                                                              min_max_norm=min_max)\r\n",
        "        x_train = training_data[:, :training_data.shape[1] - 1]\r\n",
        "        y_train = training_data[:, training_data.shape[1] - 1]\r\n",
        "        print(\"training\", x_train.shape, y_train.shape)\r\n",
        "\r\n",
        "        x_test = testing_data[:, :testing_data.shape[1] - 1]\r\n",
        "        y_test = testing_data[:, testing_data.shape[1] - 1]\r\n",
        "        print(\"testing\", x_test.shape, y_test.shape)\r\n",
        "\r\n",
        "        if plot:\r\n",
        "            plt.plot(y_train, label=\"train\")\r\n",
        "            plt.figure()\r\n",
        "            plt.plot(y_test, label=\"test\")\r\n",
        "\r\n",
        "            plt.figure()\r\n",
        "            plt.plot(x_train)\r\n",
        "            plt.title(\"train: FD00\" + str(files[0]))\r\n",
        "            plt.figure()\r\n",
        "            plt.plot(y_train)\r\n",
        "            plt.title(\"train: FD00\" + str(files[0]))\r\n",
        "            plt.show()\r\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrGWv1o8L01r"
      },
      "source": [
        "def combine_FD001_and_FD003():\r\n",
        "    column_name = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\r\n",
        "                   's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\r\n",
        "                   's15', 's16', 's17', 's18', 's19', 's20', 's21']\r\n",
        "\r\n",
        "    train_FD001 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/train_FD001.txt\", header=None, delim_whitespace=True)\r\n",
        "    train_FD003 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/train_FD001=3.txt\", header=None, delim_whitespace=True)\r\n",
        "    train_FD001.columns = column_name\r\n",
        "    train_FD003.columns = column_name\r\n",
        "\r\n",
        "    FD001_max_engine_id = max(train_FD001['engine_id'])\r\n",
        "    train_FD003['engine_id'] = train_FD003['engine_id'] + FD001_max_engine_id\r\n",
        "    train_FD003.index = range(len(train_FD001), len(train_FD001) + len(train_FD003))\r\n",
        "    train_FD001_FD002 = pd.concat([train_FD001,train_FD003])\r\n",
        "\r\n",
        "    test_FD001 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/test_FD001.txt\", header=None, delim_whitespace=True)\r\n",
        "    test_FD003 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/test_FD003.txt\", header=None, delim_whitespace=True)\r\n",
        "    test_FD001.columns = column_name\r\n",
        "    test_FD003.columns = column_name\r\n",
        "\r\n",
        "    FD001_max_engine_id = max(test_FD001['engine_id'])\r\n",
        "    test_FD003['engine_id'] = test_FD003['engine_id'] + FD001_max_engine_id\r\n",
        "    test_FD003.index = range(len(test_FD001), len(test_FD001) + len(test_FD003))\r\n",
        "    test_FD001_FD002 = pd.concat([test_FD001,test_FD003])\r\n",
        "\r\n",
        "    RUL_FD001 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/RUL_FD001.txt\", header=None, delim_whitespace=True)\r\n",
        "    RUL_FD003 = pd.read_table(\"https://raw.githubusercontent.com/ericlrf/rul/main/CMAPSSData/RUL_FD001.txt\", header=None, delim_whitespace=True)\r\n",
        "    RUL_FD001.columns = ['RUL']\r\n",
        "    RUL_FD003.columns = ['RUL']\r\n",
        "    RUL_FD003.index = range(len(RUL_FD001), len(RUL_FD001) + len(RUL_FD003))\r\n",
        "    RUL_FD001_FD002 = pd.concat([test_FD001, test_FD003])\r\n",
        "\r\n",
        "    return train_FD001_FD002,test_FD001_FD002,RUL_FD001_FD002"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvwWvjPpNqer"
      },
      "source": [
        "# utils_laj"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eOyMOmSPsT9",
        "outputId": "2a06a56a-2ddf-4bab-f918-c9a04fe51890"
      },
      "source": [
        "!pip install --upgrade tf_slim"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tf_slim in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS8Ln175N54F"
      },
      "source": [
        "import os\r\n",
        "import numpy as np\r\n",
        "# import tensorflow as tf\r\n",
        "# import tensorflow.contrib.slim as slim\r\n",
        "import tf_slim as slim\r\n",
        "# from data_processing import MAXLIFE"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ_TtQnzOOWt"
      },
      "source": [
        "def dense_layer(x, size,activation_fn, batch_norm = False,phase=False, drop_out=False, keep_prob=None, scope=\"fc_layer\"):\r\n",
        "    \"\"\"\r\n",
        "    Helper function to create a fully connected layer with or without batch normalization or dropout regularization\r\n",
        "\r\n",
        "    :param x: previous layer\r\n",
        "    :param size: fully connected layer size\r\n",
        "    :param activation_fn: activation function\r\n",
        "    :param batch_norm: bool to set batch normalization\r\n",
        "    :param phase: if batch normalization is set, then phase variable is to mention the 'training' and 'testing' phases\r\n",
        "    :param drop_out: bool to set drop-out regularization\r\n",
        "    :param keep_prob: if drop-out is set, then to mention the keep probability of dropout\r\n",
        "    :param scope: variable scope name\r\n",
        "    :return: fully connected layer\r\n",
        "    \"\"\"\r\n",
        "    with tf.variable_scope(scope):\r\n",
        "        if batch_norm:\r\n",
        "            dence_layer = tf.contrib.layers.fully_connected(x, size, activation_fn=None)\r\n",
        "            dence_layer_bn = BatchNorm(name=\"batch_norm_\" + scope)(dence_layer, train=phase)\r\n",
        "            return_layer = activation_fn(dence_layer_bn)\r\n",
        "        else:\r\n",
        "            return_layer = tf.layers.dense(x, size,\r\n",
        "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\r\n",
        "                                           activation=activation_fn)\r\n",
        "        if drop_out:\r\n",
        "            return_layer = tf.nn.dropout(return_layer, keep_prob)\r\n",
        "\r\n",
        "        return return_layer"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZHsRt_hOPK9"
      },
      "source": [
        "def get_RNNCell(cell_types, keep_prob, state_size, build_with_dropout=True):\r\n",
        "    \"\"\"\r\n",
        "    Helper function to get a different types of RNN cells with or without dropout wrapper\r\n",
        "    :param cell_types: cell_type can be 'GRU' or 'LSTM' or 'LSTM_LN' or 'GLSTMCell' or 'LSTM_BF' or 'None'\r\n",
        "    :param keep_prob: dropout keeping probability\r\n",
        "    :param state_size: number of cells in a layer\r\n",
        "    :param build_with_dropout: to enable the dropout for rnn layers\r\n",
        "    :return:\r\n",
        "    \"\"\"\r\n",
        "    cells = []\r\n",
        "    for cell_type in cell_types:\r\n",
        "        if cell_type == 'GRU':\r\n",
        "            cell = tf.contrib.rnn.GRUCell(num_units=state_size,\r\n",
        "                                          bias_initializer=tf.zeros_initializer())  # Or GRU(num_units)\r\n",
        "        elif cell_type == 'LSTM':\r\n",
        "            cell = tf.contrib.rnn.LSTMCell(num_units=state_size, use_peepholes=True, state_is_tuple=True,\r\n",
        "                                           initializer=tf.contrib.layers.xavier_initializer())\r\n",
        "        elif cell_type == 'LSTM_LN':\r\n",
        "            cell = tf.contrib.rnn.LayerNormBasicLSTMCell(state_size)\r\n",
        "        elif cell_type == 'GLSTMCell':\r\n",
        "            cell = tf.contrib.rnn.GLSTMCell(num_units=state_size, initializer=tf.contrib.layers.xavier_initializer())\r\n",
        "        elif cell_type == 'LSTM_BF':\r\n",
        "            cell = tf.contrib.rnn.LSTMBlockFusedCell(num_units=state_size, use_peephole=True)\r\n",
        "        else:\r\n",
        "            cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\r\n",
        "\r\n",
        "        if build_with_dropout:\r\n",
        "            cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\r\n",
        "        cells.append(cell)\r\n",
        "\r\n",
        "    cell = tf.contrib.rnn.MultiRNNCell(cells)\r\n",
        "\r\n",
        "    if build_with_dropout:\r\n",
        "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\r\n",
        "\r\n",
        "    return cell\r\n",
        "\r\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1dF76kgORpv"
      },
      "source": [
        "class BatchNorm(object):\r\n",
        "    \"\"\"\r\n",
        "    usage : dence_layer_bn = BatchNorm(name=\"batch_norm_\" + scope)(previous_layer, train=is_train)\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, epsilon=1e-5, momentum=0.999, name=\"batch_norm\"):\r\n",
        "        with tf.variable_scope(name):\r\n",
        "            self.epsilon = epsilon\r\n",
        "            self.momentum = momentum\r\n",
        "            self.name = name\r\n",
        "\r\n",
        "    def __call__(self, x, train=True):\r\n",
        "        return tf.contrib.layers.batch_norm(x,\r\n",
        "                                            decay=self.momentum,\r\n",
        "                                            updates_collections=None,\r\n",
        "                                            epsilon=self.epsilon,\r\n",
        "                                            scale=True,\r\n",
        "                                            is_training=train,\r\n",
        "                                            scope=self.name)\r\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jps89gQgOWJL"
      },
      "source": [
        "def batch_generator(x_train, y_train, batch_size, sequence_length, online=False, online_shift=1):\r\n",
        "    \"\"\"\r\n",
        "    Generator function for creating random batches of training-data for many to many models\r\n",
        "    \"\"\"\r\n",
        "    num_x_sensors = x_train.shape[1]\r\n",
        "    num_train = x_train.shape[0]\r\n",
        "    idx = 0\r\n",
        "\r\n",
        "    # Infinite loop.\r\n",
        "    while True:\r\n",
        "        # Allocate a new array for the batch of input-signals.\r\n",
        "        x_shape = (batch_size, sequence_length, num_x_sensors)\r\n",
        "        x_batch = np.zeros(shape=x_shape, dtype=np.float32)\r\n",
        "        # print(idx)\r\n",
        "        # Allocate a new array for the batch of output-signals.\r\n",
        "        y_shape = (batch_size, sequence_length)\r\n",
        "        y_batch = np.zeros(shape=y_shape, dtype=np.float32)\r\n",
        "\r\n",
        "        # Fill the batch with random sequences of data.\r\n",
        "        for i in range(batch_size):\r\n",
        "            # Get a random start-index.\r\n",
        "            # This points somewhere into the training-data.\r\n",
        "            if online == True and (idx >= num_train or (idx + sequence_length) > num_train):\r\n",
        "                idx = 0\r\n",
        "            elif online == False:\r\n",
        "                idx = np.random.randint(num_train - sequence_length)\r\n",
        "\r\n",
        "            # Copy the sequences of data starting at this index.\r\n",
        "            x_batch[i] = x_train[idx:idx + sequence_length]\r\n",
        "            y_batch[i] = y_train[idx:idx + sequence_length]\r\n",
        "            # print(i,idx)\r\n",
        "            if online:\r\n",
        "                idx = idx + online_shift  # check if its nee to be idx=idx+1\r\n",
        "                # print(idx)\r\n",
        "        # print(idx)\r\n",
        "        yield (x_batch, y_batch)\r\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0ftzyz3OW5V"
      },
      "source": [
        "def trjectory_generator(x_train, y_train, test_engine_id, sequence_length, graph_batch_size, lower_bound):\r\n",
        "    \"\"\"\r\n",
        "    Extract training trjectories one by one\r\n",
        "    test_engine_id = [11111111...,22222222....,...]\r\n",
        "    \"\"\"\r\n",
        "    DEBUG = False\r\n",
        "    num_x_sensors = x_train.shape[1]\r\n",
        "    idx = 0\r\n",
        "    engine_ids = test_engine_id.unique()\r\n",
        "    if DEBUG: print(\"total trjectories: \", len(engine_ids))\r\n",
        "\r\n",
        "    while True:\r\n",
        "        for id in engine_ids:\r\n",
        "\r\n",
        "            indexes = test_engine_id[test_engine_id == id].index\r\n",
        "            training_data = x_train[indexes]\r\n",
        "            if DEBUG: print(\"engine_id: \", id, \"start\", indexes[0], \"end\", indexes[-1], \"trjectory_len:\", len(indexes))\r\n",
        "            batch_size = int(training_data.shape[0] / sequence_length) + 1\r\n",
        "            idx = indexes[0]\r\n",
        "\r\n",
        "            x_batch = np.zeros(shape=(batch_size, sequence_length, num_x_sensors), dtype=np.float32)\r\n",
        "            y_batch = np.zeros(shape=(batch_size, sequence_length), dtype=np.float32)\r\n",
        "\r\n",
        "            for i in range(batch_size):\r\n",
        "\r\n",
        "                # Copy the sequences of data starting at this index.\r\n",
        "                if DEBUG: print(\"current idx=\", idx)\r\n",
        "                if idx >= x_train.shape[0]:\r\n",
        "                    if DEBUG: print(\"BREAK\")\r\n",
        "                    break\r\n",
        "                elif (idx + sequence_length) > x_train.shape[0]:\r\n",
        "                    if DEBUG: print(\"BREAK\", idx, x_train.shape[0], idx + sequence_length - x_train.shape[0])\r\n",
        "                    x_tmp = x_train[idx:]\r\n",
        "                    y_tmp = y_train[idx:]\r\n",
        "                    remain = idx + sequence_length - x_train.shape[0]\r\n",
        "                    x_batch[i] = np.concatenate((x_tmp, x_train[0:remain]))\r\n",
        "                    y_batch[i] = np.concatenate((y_tmp, y_train[0:remain]))\r\n",
        "                    break\r\n",
        "\r\n",
        "                x_batch[i] = x_train[idx:idx + sequence_length]\r\n",
        "\r\n",
        "                if idx > indexes[-1] - sequence_length:\r\n",
        "                    y_tmp = np.copy(y_train[idx:idx + sequence_length])\r\n",
        "                    remain = sequence_length - (indexes[-1] - idx + 1)  # abs(training_data.shape[0]-sequence_length)\r\n",
        "                    if DEBUG: print(\"(idx + sequence_length) > trj_len:\", \"remain\", remain)\r\n",
        "                    y_tmp[-remain:] = lower_bound\r\n",
        "                    y_batch[i] = y_tmp\r\n",
        "                else:\r\n",
        "                    y_batch[i] = y_train[idx:idx + sequence_length]\r\n",
        "\r\n",
        "                idx = idx + sequence_length\r\n",
        "\r\n",
        "            batch_size_gap = graph_batch_size - x_batch.shape[0]\r\n",
        "            if batch_size_gap > 0:\r\n",
        "                for i in range(batch_size_gap):\r\n",
        "                    x_tmp = -0.01 * np.ones(shape=(sequence_length, num_x_sensors), dtype=np.float32)\r\n",
        "                    y_tmp = -0.01 * np.ones(shape=(sequence_length), dtype=np.float32)\r\n",
        "                    xx = np.append(x_batch, x_tmp)\r\n",
        "                    x_batch = np.reshape(xx, [x_batch.shape[0] + 1, x_batch.shape[1], x_batch.shape[2]])\r\n",
        "                    yy = np.append(y_batch, y_tmp)\r\n",
        "                    y_batch = np.reshape(yy, [y_batch.shape[0] + 1, x_batch.shape[1]])\r\n",
        "            yield (x_batch, y_batch)\r\n",
        "\r\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YKrJkNXOa6Q"
      },
      "source": [
        "def plot_data(data, label=\"\"):\r\n",
        "    \"\"\"\r\n",
        "    Plot every plot on top of each other\r\n",
        "    \"\"\"\r\n",
        "    from matplotlib import pyplot as plt\r\n",
        "    if type(data) is list:\r\n",
        "        for x in data:\r\n",
        "            plt.plot(x, label=label)\r\n",
        "    else:\r\n",
        "        plt.plot(data, label=label)\r\n",
        "    plt.show()\r\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xJphXNQObuY"
      },
      "source": [
        "def model_summary(learning_rate,batch_size,lstm_layers,lstm_layer_size,fc_layer_size,sequence_length,n_channels,path_checkpoint,spacial_note=''):\r\n",
        "    path_checkpoint=path_checkpoint + \".txt\"\r\n",
        "    if not os.path.exists(os.path.dirname(path_checkpoint)):\r\n",
        "        os.makedirs(os.path.dirname(path_checkpoint))\r\n",
        "\r\n",
        "    with open(path_checkpoint, \"w\") as text_file:\r\n",
        "        variables = tf.trainable_variables()\r\n",
        "\r\n",
        "        print('---------', file=text_file)\r\n",
        "        print(path_checkpoint, file=text_file)\r\n",
        "        print(spacial_note, file=text_file)\r\n",
        "        print('---------', '\\n', file=text_file)\r\n",
        "\r\n",
        "        print('---------', file=text_file)\r\n",
        "        print('MAXLIFE: ', MAXLIFE,'\\n',  file=text_file)\r\n",
        "        print('learning_rate: ', learning_rate, file=text_file)\r\n",
        "        print('batch_size: ', batch_size, file=text_file)\r\n",
        "        print('lstm_layers: ', lstm_layers, file=text_file)\r\n",
        "        print('lstm_layer_size: ', lstm_layer_size, file=text_file)\r\n",
        "        print('fc_layer_size: ', fc_layer_size, '\\n', file=text_file)\r\n",
        "        print('sequence_length: ', sequence_length, file=text_file)\r\n",
        "        print('n_channels: ', n_channels, file=text_file)\r\n",
        "        print('---------', '\\n', file=text_file)\r\n",
        "\r\n",
        "        print('---------', file=text_file)\r\n",
        "        print('Variables: name (type shape) [size]', file=text_file)\r\n",
        "        print('---------', '\\n', file=text_file)\r\n",
        "        total_size = 0\r\n",
        "        total_bytes = 0\r\n",
        "        for var in variables:\r\n",
        "            # if var.num_elements() is None or [] assume size 0.\r\n",
        "            var_size = var.get_shape().num_elements() or 0\r\n",
        "            var_bytes = var_size * var.dtype.size\r\n",
        "            total_size += var_size\r\n",
        "            total_bytes += var_bytes\r\n",
        "            print(var.name, slim.model_analyzer.tensor_description(var), '[%d, bytes: %d]' %\r\n",
        "                      (var_size, var_bytes), file=text_file)\r\n",
        "\r\n",
        "        print('\\nTotal size of variables: %d' % total_size, file=text_file)\r\n",
        "        print('Total bytes of variables: %d' % total_bytes, file=text_file)\r\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxjr2bh4OeKb"
      },
      "source": [
        "def scoring_func(error_arr):\r\n",
        "    '''\r\n",
        "\r\n",
        "    :param error_arr: a list of errors for each training trajectory\r\n",
        "    :return: standered score value for RUL\r\n",
        "    '''\r\n",
        "    import math\r\n",
        "    # print(error_arr)\r\n",
        "    pos_error_arr = error_arr[error_arr >= 0]\r\n",
        "    neg_error_arr = error_arr[error_arr < 0]\r\n",
        "\r\n",
        "    score = 0\r\n",
        "    # print(neg_error_arr)\r\n",
        "    for error in neg_error_arr:\r\n",
        "        score = math.exp(-(error / 13)) - 1 + score\r\n",
        "        # print(math.exp(-(error / 13)),score,error)\r\n",
        "\r\n",
        "    # print(pos_error_arr)\r\n",
        "    for error in pos_error_arr:\r\n",
        "        score = math.exp(error / 10) - 1 + score\r\n",
        "        # print(math.exp(error / 10),score, error)\r\n",
        "    return score"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3KLbY4SOiLQ"
      },
      "source": [
        "def conv_layer(X,filters,kernel_size,strides,padding,batch_norm,is_train,scope):\r\n",
        "    \"\"\"\r\n",
        "    1D convolutional layer with or without dropout or batch normalization\r\n",
        "\r\n",
        "    :param batch_norm:  bool, enable batch normalization\r\n",
        "    :param is_train: bool, mention if current phase is training phase\r\n",
        "    :param scope: variable scope\r\n",
        "    :return: 1D-convolutional layer\r\n",
        "    \"\"\"\r\n",
        "    with tf.variable_scope(scope):\r\n",
        "        if batch_norm:\r\n",
        "            conv1 = tf.layers.conv1d(inputs=X, filters=filters, kernel_size=kernel_size, strides=strides,\r\n",
        "                                     padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer())\r\n",
        "            return tf.nn.relu(BatchNorm(name=\"norm_\"+scope)(conv1, train=is_train))\r\n",
        "        else:\r\n",
        "            return tf.layers.conv1d(inputs=X, filters=filters, kernel_size=kernel_size, strides=strides,\r\n",
        "                                     padding=padding, kernel_initializer=tf.contrib.layers.xavier_initializer(),\r\n",
        "                                     activation=tf.nn.relu)\r\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M9paJ2DOkhY"
      },
      "source": [
        "def get_predicted_expected_RUL(__y, __y_pred, lower_bound=-1):\r\n",
        "    trj_end = np.argmax(__y == lower_bound) - 1\r\n",
        "    trj_pred = __y_pred[:trj_end]\r\n",
        "    trj_pred[trj_pred < 0] = 0\r\n",
        "    # if trj_pred[-1] < 0: print(trj_pred[-1])\r\n",
        "    RUL_predict = round(trj_pred[-1], 0)\r\n",
        "    RUL_expected = round(__y[trj_end], 0)\r\n",
        "\r\n",
        "    return RUL_predict, RUL_expected"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA4Yqp2fZgJ1"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5tpNNCdOmyY"
      },
      "source": [
        "# import tensorflow.compat.v1 as tf\r\n",
        "# tf.disable_v2_behavior()\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import time\r\n",
        "import datetime\r\n",
        "# from utils_laj import *\r\n",
        "# from data_processing import get_CMAPSSData, get_PHM08Data, data_augmentation, analyse_Data\r\n",
        "\r\n",
        "today = datetime.date.today()\r\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjWJbL8dO4Sd",
        "outputId": "c46f0abf-567e-4f42-9255-1164d5149875"
      },
      "source": [
        "def CNNLSTM(dataset, file_no, Train=False, trj_wise=False, plot=False):\r\n",
        "    '''\r\n",
        "    The architecture is a Meny-to-meny model combining CNN and LSTM models\r\n",
        "    :param dataset: select the specific dataset between PHM08 or CMAPSS\r\n",
        "    :param Train: select between training and testing\r\n",
        "    :param trj_wise: Trajectorywise calculate RMSE and scores\r\n",
        "    '''\r\n",
        "\r\n",
        "    #### checkpoint saving path ####\r\n",
        "    if file_no == 1:\r\n",
        "        path_checkpoint = './Save/Save_CNNLSTM/CNNLSTM_ML120_GRAD1_kinkRUL_FD001/CNN1D_3_lstm_2_layers'\r\n",
        "    elif file_no == 2:\r\n",
        "        path_checkpoint = './Save/Save_CNNLSTM/CNNLSTM_ML120_GRAD1_kinkRUL_FD002/CNN1D_3_lstm_2_layers'\r\n",
        "    elif file_no == 3:\r\n",
        "        path_checkpoint = './Save/Save_CNNLSTM/CNNLSTM_ML120_GRAD1_kinkRUL_FD003/CNN1D_3_lstm_2_layers'\r\n",
        "    elif file_no == 4:\r\n",
        "        path_checkpoint = './Save/Save_CNNLSTM/CNNLSTM_ML120_GRAD1_kinkRUL_FD004/CNN1D_3_lstm_2_layers'\r\n",
        "    else:\r\n",
        "        raise ValueError(\"Save path not defined\")\r\n",
        "    ##################################\r\n",
        "\r\n",
        "\r\n",
        "    if dataset == \"cmapss\":\r\n",
        "        training_data, testing_data, training_pd, testing_pd = get_CMAPSSData(save=False)\r\n",
        "        x_train = training_data[:, :training_data.shape[1] - 1]\r\n",
        "        y_train = training_data[:, training_data.shape[1] - 1]\r\n",
        "        print(\"training data CNNLSTM: \", x_train.shape, y_train.shape)\r\n",
        "\r\n",
        "        x_test = testing_data[:, :testing_data.shape[1] - 1]\r\n",
        "        y_test = testing_data[:, testing_data.shape[1] - 1]\r\n",
        "        print(\"testing data CNNLSTM: \", x_test.shape, y_test.shape)\r\n",
        "\r\n",
        "    elif dataset == \"phm\":\r\n",
        "        training_data, testing_data, phm_testing_data = get_PHM08Data(save=False)\r\n",
        "        x_validation = phm_testing_data[:, :phm_testing_data.shape[1] - 1]\r\n",
        "        y_validation = phm_testing_data[:, phm_testing_data.shape[1] - 1]\r\n",
        "        print(\"testing data: \", x_validation.shape, y_validation.shape)\r\n",
        "\r\n",
        "    batch_size = 1024  # Batch size\r\n",
        "    if Train == False: batch_size = 5\r\n",
        "\r\n",
        "    sequence_length = 100  # Number of steps\r\n",
        "    learning_rate = 0.001  # 0.0001\r\n",
        "    epochs = 5000\r\n",
        "    ann_hidden = 50\r\n",
        "\r\n",
        "    n_channels = 24\r\n",
        "\r\n",
        "    lstm_size = n_channels * 3  # 3 times the amount of channels\r\n",
        "    num_layers = 2  # 2  # Number of layers\r\n",
        "\r\n",
        "    X = tf.placeholder(tf.float32, [None, sequence_length, n_channels], name='inputs')\r\n",
        "    Y = tf.placeholder(tf.float32, [None, sequence_length], name='labels')\r\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\r\n",
        "    learning_rate_ = tf.placeholder(tf.float32, name='learning_rate')\r\n",
        "    is_train = tf.placeholder(dtype=tf.bool, shape=None, name=\"is_train\")\r\n",
        "\r\n",
        "    conv1 = conv_layer(X, filters=18, kernel_size=2, strides=1, padding='same', batch_norm=False, is_train=is_train,\r\n",
        "                       scope='conv_1')\r\n",
        "    max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=2, padding='same', name='maxpool_1')\r\n",
        "\r\n",
        "    conv2 = conv_layer(max_pool_1, filters=36, kernel_size=2, strides=1, padding='same', batch_norm=False,\r\n",
        "                       is_train=is_train, scope='conv_2')\r\n",
        "    max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=2, padding='same', name='maxpool_2')\r\n",
        "\r\n",
        "    conv3 = conv_layer(max_pool_2, filters=72, kernel_size=2, strides=1, padding='same', batch_norm=False,\r\n",
        "                       is_train=is_train, scope='conv_3')\r\n",
        "    max_pool_3 = tf.layers.max_pooling1d(inputs=conv3, pool_size=2, strides=2, padding='same', name='maxpool_3')\r\n",
        "\r\n",
        "    conv_last_layer = max_pool_3\r\n",
        "\r\n",
        "    shape = conv_last_layer.get_shape().as_list()\r\n",
        "    CNN_flat = tf.reshape(conv_last_layer, [-1, shape[1] * shape[2]])\r\n",
        "\r\n",
        "    dence_layer_1 = dense_layer(CNN_flat, size=sequence_length * n_channels, activation_fn=tf.nn.relu, batch_norm=False,\r\n",
        "                                phase=is_train, drop_out=True, keep_prob=keep_prob,\r\n",
        "                                scope=\"fc_1\")\r\n",
        "    lstm_input = tf.reshape(dence_layer_1, [-1, sequence_length, n_channels])\r\n",
        "\r\n",
        "    cell = get_RNNCell(['LSTM'] * num_layers, keep_prob=keep_prob, state_size=lstm_size)\r\n",
        "    init_state = cell.zero_state(batch_size, tf.float32)\r\n",
        "    rnn_output, states = tf.nn.dynamic_rnn(cell, lstm_input, dtype=tf.float32, initial_state=init_state)\r\n",
        "    stacked_rnn_output = tf.reshape(rnn_output, [-1, lstm_size])  # change the form into a tensor\r\n",
        "\r\n",
        "    dence_layer_2 = dense_layer(stacked_rnn_output, size=ann_hidden, activation_fn=tf.nn.relu, batch_norm=False,\r\n",
        "                                phase=is_train, drop_out=True, keep_prob=keep_prob,\r\n",
        "                                scope=\"fc_2\")\r\n",
        "\r\n",
        "    output = dense_layer(dence_layer_2, size=1, activation_fn=None, batch_norm=False, phase=is_train, drop_out=False,\r\n",
        "                         keep_prob=keep_prob,\r\n",
        "                         scope=\"fc_3_output\")\r\n",
        "\r\n",
        "    prediction = tf.reshape(output, [-1])\r\n",
        "    y_flat = tf.reshape(Y, [-1])\r\n",
        "\r\n",
        "    h = prediction - y_flat\r\n",
        "\r\n",
        "    cost_function = tf.reduce_sum(tf.square(h))\r\n",
        "    RMSE = tf.sqrt(tf.reduce_mean(tf.square(h)))\r\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost_function)\r\n",
        "\r\n",
        "    saver = tf.train.Saver()\r\n",
        "    training_generator = batch_generator(x_train, y_train, batch_size, sequence_length, online=True)\r\n",
        "    testing_generator = batch_generator(x_test, y_test, batch_size, sequence_length, online=False)\r\n",
        "\r\n",
        "    if Train: model_summary(learning_rate=learning_rate, batch_size=batch_size, lstm_layers=num_layers,\r\n",
        "                            lstm_layer_size=lstm_size, fc_layer_size=ann_hidden, sequence_length=sequence_length,\r\n",
        "                            n_channels=n_channels, path_checkpoint=path_checkpoint, spacial_note='')\r\n",
        "\r\n",
        "    with tf.Session() as session:\r\n",
        "        tf.global_variables_initializer().run()\r\n",
        "\r\n",
        "        if Train == True:\r\n",
        "            cost = []\r\n",
        "            iteration = int(x_train.shape[0] / batch_size)\r\n",
        "            print(\"Training set MSE\")\r\n",
        "            print(\"No epoches: \", epochs, \"No itr: \", iteration)\r\n",
        "            __start = time.time()\r\n",
        "            for ep in range(epochs):\r\n",
        "\r\n",
        "                for itr in range(iteration):\r\n",
        "                    ## training ##\r\n",
        "                    batch_x, batch_y = next(training_generator)\r\n",
        "                    session.run(optimizer,\r\n",
        "                                feed_dict={X: batch_x, Y: batch_y, keep_prob: 0.8, learning_rate_: learning_rate})\r\n",
        "                    cost.append(\r\n",
        "                        RMSE.eval(feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0, learning_rate_: learning_rate}))\r\n",
        "\r\n",
        "                x_test_batch, y_test_batch = next(testing_generator)\r\n",
        "                mse_train, rmse_train = session.run([cost_function, RMSE],\r\n",
        "                                                    feed_dict={X: batch_x, Y: batch_y, keep_prob: 1.0,\r\n",
        "                                                               learning_rate_: learning_rate})\r\n",
        "                mse_test, rmse_test = session.run([cost_function, RMSE],\r\n",
        "                                                  feed_dict={X: x_test_batch, Y: y_test_batch, keep_prob: 1.0,\r\n",
        "                                                             learning_rate_: learning_rate})\r\n",
        "\r\n",
        "                time_per_ep = (time.time() - __start)\r\n",
        "                time_remaining = ((epochs - ep) * time_per_ep) / 3600\r\n",
        "                print(\"CNNLSTM\", \"epoch:\", ep, \"\\tTrainig-\",\r\n",
        "                      \"MSE:\", mse_train, \"RMSE:\", rmse_train, \"\\tTesting-\", \"MSE\", mse_test, \"RMSE\", rmse_test,\r\n",
        "                      \"\\ttime/epoch:\", round(time_per_ep, 2), \"\\ttime_remaining: \",\r\n",
        "                      int(time_remaining), \" hr:\", round((time_remaining % 1) * 60, 1), \" min\", \"\\ttime_stamp: \",\r\n",
        "                      datetime.datetime.now().strftime(\"%Y.%m.%d-%H:%M:%S\"))\r\n",
        "                __start = time.time()\r\n",
        "\r\n",
        "                if ep % 10 == 0 and ep != 0:\r\n",
        "                    save_path = saver.save(session, path_checkpoint)\r\n",
        "                    if os.path.exists(path_checkpoint + '.meta'):\r\n",
        "                        print(\"Model saved to file: %s\" % path_checkpoint)\r\n",
        "                    else:\r\n",
        "                        print(\"NOT SAVED!!!\", path_checkpoint)\r\n",
        "\r\n",
        "                if ep % 1000 == 0 and ep != 0: learning_rate = learning_rate / 10\r\n",
        "\r\n",
        "            save_path = saver.save(session, path_checkpoint)\r\n",
        "            if os.path.exists(path_checkpoint + '.meta'):\r\n",
        "                print(\"Model saved to file: %s\" % path_checkpoint)\r\n",
        "            else:\r\n",
        "                print(\"NOT SAVED!!!\", path_checkpoint)\r\n",
        "            plt.plot(cost)\r\n",
        "            plt.show()\r\n",
        "        else:\r\n",
        "            saver.restore(session, path_checkpoint)\r\n",
        "            print(\"Model restored from file: %s\" % path_checkpoint)\r\n",
        "\r\n",
        "            if trj_wise:\r\n",
        "                trj_iteration = len(test_engine_id.unique())\r\n",
        "                print(\"total trajectories: \", trj_iteration)\r\n",
        "                error_list = []\r\n",
        "                pred_list = []\r\n",
        "                expected_list = []\r\n",
        "                lower_bound = -0.01\r\n",
        "                test_trjectory_generator = trjectory_generator(x_test, y_test, test_engine_id, sequence_length,\r\n",
        "                                                               batch_size, lower_bound)\r\n",
        "                for itr in range(trj_iteration):\r\n",
        "                    trj_x, trj_y = next(test_trjectory_generator)\r\n",
        "\r\n",
        "                    __y_pred, error, __y = session.run([prediction, h, y_flat],\r\n",
        "                                                       feed_dict={X: trj_x, Y: trj_y, keep_prob: 1.0})\r\n",
        "\r\n",
        "                    RUL_predict, RUL_expected = get_predicted_expected_RUL(__y, __y_pred, lower_bound)\r\n",
        "\r\n",
        "                    error_list.append(RUL_predict - RUL_expected)\r\n",
        "                    pred_list.append(RUL_predict)\r\n",
        "                    expected_list.append(RUL_expected)\r\n",
        "\r\n",
        "                    print(\"id: \", itr + 1, \"expected: \", RUL_expected, \"\\t\", \"predict: \", RUL_predict, \"\\t\", \"error: \",\r\n",
        "                          RUL_predict - RUL_expected)\r\n",
        "                    # plt.plot(__y_pred* RESCALE, label=\"prediction\")\r\n",
        "                    # plt.plot(__y* RESCALE, label=\"expected\")\r\n",
        "                    # plt.show()\r\n",
        "                error_list = np.array(error_list)\r\n",
        "                error_list = error_list.ravel()\r\n",
        "                rmse = np.sqrt(np.sum(np.square(error_list)) / len(error_list))  # RMSE\r\n",
        "                print(rmse, scoring_func(error_list))\r\n",
        "                if plot:\r\n",
        "                    plt.figure()\r\n",
        "                    # plt.plot(expected_list, 'o', color='black', label=\"expected\")\r\n",
        "                    # plt.plot(pred_list, 'o', color='red', label=\"predicted\")\r\n",
        "                    # plt.figure()\r\n",
        "                    plt.plot(np.sort(error_list), 'o', color='red', label=\"error\")\r\n",
        "                    plt.legend()\r\n",
        "                    plt.show()\r\n",
        "                fig, ax = plt.subplots()\r\n",
        "                ax.stem(expected_list, linefmt='b-', label=\"expected\")\r\n",
        "                ax.stem(pred_list, linefmt='r-', label=\"predicted\")\r\n",
        "                plt.legend()\r\n",
        "                plt.show()\r\n",
        "\r\n",
        "            else:\r\n",
        "                x_validation = x_test\r\n",
        "                y_validation = y_test\r\n",
        "\r\n",
        "                validation_generator = batch_generator(x_validation, y_validation, batch_size, sequence_length,\r\n",
        "                                                       online=True,\r\n",
        "                                                       online_shift=sequence_length)\r\n",
        "\r\n",
        "                full_prediction = []\r\n",
        "                actual_rul = []\r\n",
        "                error_list = []\r\n",
        "\r\n",
        "                iteration = int(x_validation.shape[0] / (batch_size * sequence_length))\r\n",
        "                print(\"#of validation points:\", x_validation.shape[0], \"#datapoints covers from minibatch:\",\r\n",
        "                      batch_size * sequence_length, \"iterations/epoch\", iteration)\r\n",
        "\r\n",
        "                for itr in range(iteration):\r\n",
        "                    x_validate_batch, y_validate_batch = next(validation_generator)\r\n",
        "                    __y_pred, error, __y = session.run([prediction, h, y_flat],\r\n",
        "                                                       feed_dict={X: x_validate_batch, Y: y_validate_batch,\r\n",
        "                                                                  keep_prob: 1.0})\r\n",
        "                    full_prediction.append(__y_pred * RESCALE)\r\n",
        "                    actual_rul.append(__y * RESCALE)\r\n",
        "                    error_list.append(error * RESCALE)\r\n",
        "                full_prediction = np.array(full_prediction)\r\n",
        "                full_prediction = full_prediction.ravel()\r\n",
        "                actual_rul = np.array(actual_rul)\r\n",
        "                actual_rul = actual_rul.ravel()\r\n",
        "                error_list = np.array(error_list)\r\n",
        "                error_list = error_list.ravel()\r\n",
        "                rmse = np.sqrt(np.sum(np.square(error_list)) / len(error_list))  # RMSE\r\n",
        "\r\n",
        "                print(y_validation.shape, full_prediction.shape, \"RMSE:\", rmse, \"Score:\", scoring_func(error_list))\r\n",
        "                if plot:\r\n",
        "                    plt.plot(full_prediction, label=\"prediction\")\r\n",
        "                    plt.plot(actual_rul, label=\"expected\")\r\n",
        "                    plt.legend()\r\n",
        "                    plt.show()\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "\r\n",
        "    dataset = \"cmapss\" \r\n",
        "    file = 1 # represent the sub-dataset for cmapss\r\n",
        "    TRAIN = True\r\n",
        "    TRJ_WISE = True\r\n",
        "    PLOT = True\r\n",
        "\r\n",
        "    analyse_Data(dataset=dataset, files=[file], plot=False, min_max=False)\r\n",
        "\r\n",
        "    if TRAIN: data_augmentation(files=file,\r\n",
        "                                low=[10, 35, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250, 270, 290, 310, 330],\r\n",
        "                                high=[35, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250, 270, 290, 310, 330, 350],\r\n",
        "                                plot=False,\r\n",
        "                                combine=False)\r\n",
        "\r\n",
        "    # from data_processing import RESCALE, test_engine_id\r\n",
        "\r\n",
        "    CNNLSTM(dataset=dataset, file_no=file, Train=TRAIN, trj_wise=TRJ_WISE, plot=PLOT)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_FD001\n",
            "test_FD001\n",
            "training (20631, 24) (20631,)\n",
            "testing (13096, 24) (13096,)\n",
            "train_FD001.txt\n",
            "************* (20631, 26) 100 10 35 *****************\n",
            "************* (22990, 27) 100 35 50 *****************\n",
            "************* (27285, 27) 100 50 70 *****************\n",
            "************* (33266, 27) 100 70 90 *****************\n",
            "************* (41238, 27) 100 90 110 *****************\n",
            "************* (51182, 27) 100 110 130 *****************\n",
            "************* (63122, 27) 100 130 150 *****************\n",
            "************* (77085, 27) 100 150 170 *****************\n",
            "************* (92860, 27) 100 170 190 *****************\n",
            "************* (110246, 27) 100 190 210 *****************\n",
            "************* (128854, 27) 100 210 230 *****************\n",
            "************* (148180, 27) 100 230 250 *****************\n",
            "************* (168034, 27) 100 250 270 *****************\n",
            "************* (188169, 27) 100 270 290 *****************\n",
            "************* (208501, 27) 100 290 310 *****************\n",
            "************* (228966, 27) 100 310 330 *****************\n",
            "************* (249505, 27) 100 330 350 *****************\n",
            "training in augmentation (270105, 24) (270105,)\n",
            "training data CNNLSTM:  (270105, 24) (270105,)\n",
            "testing data CNNLSTM:  (13096, 24) (13096,)\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-45-42d5b086590d>:18: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv1D` instead.\n",
            "WARNING:tensorflow:Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f4ae9df2d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f4ae9df2d90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f4ae9df2d90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f4ae9df2d90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From <ipython-input-48-90cb4afb277f>:60: max_pooling1d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling1D instead.\n",
            "WARNING:tensorflow:Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f4aedea6090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f4aedea6090>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f4aedea6090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f4aedea6090>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f4ae9bf7f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f4ae9bf7f10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f4ae9bf7f10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f4ae9bf7f10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f4aedea6090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f4aedea6090>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f4aedea6090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f4aedea6090>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f4ae9d5d8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f4ae9d5d8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f4ae9d5d8d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv1D.call of <tensorflow.python.layers.convolutional.Conv1D object at 0x7f4ae9d5d8d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f4aedea6090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f4aedea6090>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f4aedea6090>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling1D.call of <tensorflow.python.layers.pooling.MaxPooling1D object at 0x7f4aedea6090>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From <ipython-input-37-9ea282c8be5a>:23: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4ae9c96750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4ae9c96750>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4ae9c96750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4ae9c96750>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From <ipython-input-37-9ea282c8be5a>:25: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-38-af893cf778c5>:17: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-38-af893cf778c5>:31: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-48-90cb4afb277f>:82: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f4aecdef790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f4aecdef790>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f4aecdef790>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f4aecdef790>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f4aece03fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f4aece03fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f4aece03fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f4aece03fd0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f4ae9af7550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f4ae9af7550>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f4ae9af7550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f4ae9af7550>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4ae9abb750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4ae9abb750>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4ae9abb750>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4ae9abb750>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4ae99cb150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4ae99cb150>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4ae99cb150>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f4ae99cb150>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "Training set MSE\n",
            "No epoches:  5000 No itr:  263\n",
            "CNNLSTM epoch: 0 \tTrainig- MSE: 139958750.0 RMSE: 36.97005 \tTesting- MSE 94587360.0 RMSE 30.392511 \ttime/epoch: 693.8 \ttime_remaining:  963  hr: 37.0  min \ttime_stamp:  2021.02.25-23:28:43\n",
            "CNNLSTM epoch: 1 \tTrainig- MSE: 7775696.0 RMSE: 8.714043 \tTesting- MSE 40831480.0 RMSE 19.9686 \ttime/epoch: 700.78 \ttime_remaining:  973  hr: 6.4  min \ttime_stamp:  2021.02.25-23:40:24\n",
            "CNNLSTM epoch: 2 \tTrainig- MSE: 8104907.0 RMSE: 8.8966 \tTesting- MSE 31971036.0 RMSE 17.669668 \ttime/epoch: 702.93 \ttime_remaining:  975  hr: 54.1  min \ttime_stamp:  2021.02.25-23:52:07\n",
            "CNNLSTM epoch: 3 \tTrainig- MSE: 6089094.0 RMSE: 7.711278 \tTesting- MSE 30555506.0 RMSE 17.274073 \ttime/epoch: 718.46 \ttime_remaining:  997  hr: 15.5  min \ttime_stamp:  2021.02.26-00:04:05\n",
            "CNNLSTM epoch: 4 \tTrainig- MSE: 3229338.8 RMSE: 5.615738 \tTesting- MSE 31454566.0 RMSE 17.526365 \ttime/epoch: 716.53 \ttime_remaining:  994  hr: 23.2  min \ttime_stamp:  2021.02.26-00:16:02\n",
            "CNNLSTM epoch: 5 \tTrainig- MSE: 4285964.5 RMSE: 6.4695535 \tTesting- MSE 26935448.0 RMSE 16.218554 \ttime/epoch: 717.81 \ttime_remaining:  995  hr: 57.3  min \ttime_stamp:  2021.02.26-00:27:59\n",
            "CNNLSTM epoch: 6 \tTrainig- MSE: 5121249.0 RMSE: 7.07193 \tTesting- MSE 27578640.0 RMSE 16.411053 \ttime/epoch: 718.82 \ttime_remaining:  997  hr: 10.1  min \ttime_stamp:  2021.02.26-00:39:58\n",
            "CNNLSTM epoch: 7 \tTrainig- MSE: 5837775.5 RMSE: 7.550465 \tTesting- MSE 31313880.0 RMSE 17.487127 \ttime/epoch: 723.08 \ttime_remaining:  1002  hr: 52.1  min \ttime_stamp:  2021.02.26-00:52:01\n",
            "CNNLSTM epoch: 8 \tTrainig- MSE: 5452207.5 RMSE: 7.296863 \tTesting- MSE 25937168.0 RMSE 15.915171 \ttime/epoch: 715.18 \ttime_remaining:  991  hr: 42.7  min \ttime_stamp:  2021.02.26-01:03:57\n",
            "CNNLSTM epoch: 9 \tTrainig- MSE: 3214787.5 RMSE: 5.603071 \tTesting- MSE 26616776.0 RMSE 16.122328 \ttime/epoch: 716.85 \ttime_remaining:  993  hr: 49.6  min \ttime_stamp:  2021.02.26-01:15:53\n",
            "CNNLSTM epoch: 10 \tTrainig- MSE: 3403286.5 RMSE: 5.7649994 \tTesting- MSE 26354908.0 RMSE 16.042822 \ttime/epoch: 718.28 \ttime_remaining:  995  hr: 37.3  min \ttime_stamp:  2021.02.26-01:27:52\n",
            "Model saved to file: ./Save/Save_CNNLSTM/CNNLSTM_ML120_GRAD1_kinkRUL_FD001/CNN1D_3_lstm_2_layers\n",
            "CNNLSTM epoch: 11 \tTrainig- MSE: 5295973.0 RMSE: 7.1915565 \tTesting- MSE 26108774.0 RMSE 15.967732 \ttime/epoch: 721.5 \ttime_remaining:  999  hr: 52.7  min \ttime_stamp:  2021.02.26-01:39:53\n",
            "CNNLSTM epoch: 12 \tTrainig- MSE: 3267591.5 RMSE: 5.6489 \tTesting- MSE 26333088.0 RMSE 16.03618 \ttime/epoch: 705.8 \ttime_remaining:  977  hr: 55.2  min \ttime_stamp:  2021.02.26-01:51:39\n",
            "CNNLSTM epoch: 13 \tTrainig- MSE: 2025984.6 RMSE: 4.448034 \tTesting- MSE 27367060.0 RMSE 16.34798 \ttime/epoch: 702.32 \ttime_remaining:  972  hr: 54.1  min \ttime_stamp:  2021.02.26-02:03:21\n",
            "CNNLSTM epoch: 14 \tTrainig- MSE: 1613439.9 RMSE: 3.9694142 \tTesting- MSE 31047364.0 RMSE 17.41255 \ttime/epoch: 701.65 \ttime_remaining:  971  hr: 47.0  min \ttime_stamp:  2021.02.26-02:15:03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4Ae_VSyeMoc"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}